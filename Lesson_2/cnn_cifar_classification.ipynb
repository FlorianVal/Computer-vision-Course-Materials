{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c446910c",
   "metadata": {},
   "source": [
    "# Lesson 2: Building and Training a CNN for Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will build and train a Convolutional Neural Network (CNN) to classify images from the CIFAR-10 and CIFAR-100 datasets. We'll follow the typical data science pipeline:\n",
    "\n",
    "1. Load and explore the data\n",
    "2. Analyze the dataset\n",
    "3. Implement data augmentation\n",
    "4. Build a CNN model\n",
    "5. Train and evaluate the model\n",
    "6. Experiment with different architectures\n",
    "7. Scale to a more complex dataset (CIFAR-100)\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "# Check for CUDA (NVIDIA GPU) or MPS (Apple Silicon) availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab62a3",
   "metadata": {},
   "source": [
    "Now, let's load the CIFAR-10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68765f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic transformations for loading the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 training and test datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes in CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a75a6",
   "metadata": {},
   "source": [
    "Let's visualize some of the images from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "print('Labels:', ' '.join(f'{classes[labels[j]]:5s}' for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36c05d",
   "metadata": {},
   "source": [
    "## 2. Data Analysis\n",
    "\n",
    "Now, let's analyze the dataset to better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703e55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function to analyze the dataset\n",
    "# This function should output metrics about the dataset such as:\n",
    "# - Number of samples in training and test sets\n",
    "# - Number of samples per class\n",
    "# - Size of one data sample (image dimensions)\n",
    "\n",
    "def analyze_dataset(trainset, testset, classes):\n",
    "    \"\"\"\n",
    "    Analyze the dataset and print useful information.\n",
    "    \n",
    "    Args:\n",
    "        trainset: The training dataset\n",
    "        testset: The test dataset\n",
    "        classes: List of class names\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: You can use trainset.data to access the raw data\n",
    "    \n",
    "    # 1. Get the total number of samples\n",
    "    \n",
    "    # 2. Get the image dimensions\n",
    "    \n",
    "    # 3. Count samples per class in the training set\n",
    "    \n",
    "    # 4. Visualize the class distribution\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Call the function to analyze the dataset\n",
    "analyze_dataset(trainset, testset, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e04363",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation\n",
    "\n",
    "Data augmentation is a technique to artificially increase the size of the training dataset by applying various transformations to the original images. This helps improve the model's generalization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45635334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement a function for data augmentation\n",
    "# This function should apply different transformations to the images\n",
    "# For more information on available transforms, see:\n",
    "# https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "def get_augmentation_transforms():\n",
    "    \"\"\"\n",
    "    Create a set of data augmentation transforms.\n",
    "    \n",
    "    Returns:\n",
    "        A torchvision.transforms.Compose object with various augmentations\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: Use transforms like RandomHorizontalFlip, RandomRotation, ColorJitter, etc.\n",
    "    \n",
    "    augmentation_transforms = transforms.Compose([\n",
    "        # Add your transformations here\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    return augmentation_transforms\n",
    "\n",
    "# Create a new training dataset with augmentation\n",
    "augmentation_transforms = get_augmentation_transforms()\n",
    "augmented_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                                download=True, transform=augmentation_transforms)\n",
    "augmented_trainloader = torch.utils.data.DataLoader(augmented_trainset, batch_size=64,\n",
    "                                                  shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedd155",
   "metadata": {},
   "source": [
    "Let's visualize some of the augmented images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ab22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random augmented training images\n",
    "dataiter = iter(augmented_trainloader)\n",
    "augmented_images, augmented_labels = next(dataiter)\n",
    "\n",
    "# Show original images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print('Original Labels:', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "# Show augmented images\n",
    "imshow(torchvision.utils.make_grid(augmented_images[:4]))\n",
    "print('Augmented Labels:', ' '.join(f'{classes[augmented_labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00880330",
   "metadata": {},
   "source": [
    "## 4. Building a Simple CNN\n",
    "\n",
    "Now, let's build a simple CNN model for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7784839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a minimal example of a CNN with one convolutional layer and a fully connected layer\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
    "        x = x.view(-1, 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "print(model)\n",
    "# infer one data with the model\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(f\"Predicted: {predicted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079ab88",
   "metadata": {},
   "source": [
    "## 5. Training the Model\n",
    "\n",
    "Now, let's implement the training loop for our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2207a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the training loop\n",
    "# This function should train the model on the training data and evaluate it on the test data\n",
    "\n",
    "# Documentation links:\n",
    "# - PyTorch optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "# - PyTorch loss functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "def train_model(model, trainloader, testloader, criterion, optimizer, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network model\n",
    "        trainloader: DataLoader for training data\n",
    "        testloader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        num_epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        Lists of training losses, training accuracies, and test accuracies\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: A typical training loop includes:\n",
    "    # 1. Iterate through batches of training data\n",
    "    # 2. Zero the gradients\n",
    "    # 3. Forward pass\n",
    "    # 4. Compute loss\n",
    "    # 5. Backward pass\n",
    "    # 6. Update weights\n",
    "    # 7. Evaluate on test data\n",
    "    \n",
    "    # Initialize lists to track metrics\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Your training code here\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        # Your evaluation code here\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Acc: {train_acc:.2f}%, '\n",
    "              f'Test Acc: {test_acc:.2f}%')\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "    \n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "train_losses, train_accuracies, test_accuracies = train_model(\n",
    "    model, augmented_trainloader, testloader, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d7ae7",
   "metadata": {},
   "source": [
    "Let's visualize the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and test metrics\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d2b821",
   "metadata": {},
   "source": [
    "## 6. Visualizing Learned Filters\n",
    "\n",
    "Let's visualize the filters (weights) learned by the convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5870171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the first convolutional layer\n",
    "weights = model.conv1.weight.data.cpu()\n",
    "\n",
    "# Plot the filters\n",
    "plt.figure(figsize=(12, 6))\n",
    "num_filters = weights.shape[0]  # Get actual number of filters\n",
    "\n",
    "# Create a grid for visualization\n",
    "rows = 2\n",
    "cols = 3\n",
    "for i in range(num_filters):\n",
    "    if i >= rows * cols:\n",
    "        break  # Limit the number of filters to display\n",
    "    \n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    \n",
    "    # For RGB filters, we'll use a single channel representation for simplicity\n",
    "    # We'll use the average of the 3 channels\n",
    "    filter_data = weights[i].mean(dim=0).numpy()\n",
    "    \n",
    "    # Display the filter\n",
    "    im = plt.imshow(filter_data, cmap='viridis')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Filter {i+1}')\n",
    "\n",
    "# Add a colorbar with gradient legend showing actual weight values\n",
    "cax = plt.axes([0.92, 0.2, 0.02, 0.6])  # [left, bottom, width, height]\n",
    "cmap = plt.cm.viridis\n",
    "norm = plt.Normalize(weights.min().item(), weights.max().item())\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)\n",
    "cbar.set_label('Weight Value')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 0.95])  # Adjust layout to make room for colorbar\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab1ab6",
   "metadata": {},
   "source": [
    "## 7. Improving the Model\n",
    "\n",
    "Now, let's try to improve our model by adding more layers and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ce6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement an improved CNN model\n",
    "# This model should have more convolutional layers and possibly other improvements\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        # Your code here\n",
    "        # Hint: Consider adding more convolutional layers, batch normalization,\n",
    "        # dropout, or other techniques to improve the model\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "# Create an instance of the improved model\n",
    "improved_model = ImprovedCNN(num_classes=10).to(device)\n",
    "print(improved_model)\n",
    "\n",
    "# Define loss function and optimizer for the improved model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(improved_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the improved model\n",
    "improved_train_losses, improved_train_accuracies, improved_test_accuracies = train_model(\n",
    "    improved_model, augmented_trainloader, testloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a1e09",
   "metadata": {},
   "source": [
    "Let's compare the performance of our simple and improved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of test accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_accuracies, label='Simple CNN')\n",
    "plt.plot(improved_test_accuracies, label='Improved CNN')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6aaeb",
   "metadata": {},
   "source": [
    "## 8. Scaling to CIFAR-100\n",
    "\n",
    "Now, let's adapt our model to work with the more complex CIFAR-100 dataset, which has 100 classes instead of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64dbf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "cifar100_trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                                download=True, transform=augmentation_transforms)\n",
    "cifar100_testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                               download=True, transform=transform)\n",
    "\n",
    "cifar100_trainloader = torch.utils.data.DataLoader(cifar100_trainset, batch_size=64,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "cifar100_testloader = torch.utils.data.DataLoader(cifar100_testset, batch_size=64,\n",
    "                                                shuffle=False, num_workers=2)\n",
    "\n",
    "# TODO: Adapt your improved model for CIFAR-100\n",
    "# You'll need to modify the final layer to output 100 classes instead of 10\n",
    "\n",
    "# Create an instance of the model for CIFAR-100\n",
    "cifar100_model = ImprovedCNN(num_classes=100).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cifar100_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model on CIFAR-100\n",
    "cifar100_train_losses, cifar100_train_accuracies, cifar100_test_accuracies = train_model(\n",
    "    cifar100_model, cifar100_trainloader, cifar100_testloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372d271",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation and Comparison\n",
    "\n",
    "Let's evaluate our models on the test sets and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96683c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model on a dataset\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate models\n",
    "simple_cifar10_accuracy = evaluate_model(model, testloader, device)\n",
    "improved_cifar10_accuracy = evaluate_model(improved_model, testloader, device)\n",
    "cifar100_accuracy = evaluate_model(cifar100_model, cifar100_testloader, device)\n",
    "\n",
    "print(f\"Simple CNN on CIFAR-10: {simple_cifar10_accuracy:.2f}% accuracy\")\n",
    "print(f\"Improved CNN on CIFAR-10: {improved_cifar10_accuracy:.2f}% accuracy\")\n",
    "print(f\"Improved CNN on CIFAR-100: {cifar100_accuracy:.2f}% accuracy\")\n",
    "\n",
    "# Plot final comparison\n",
    "accuracies = [simple_cifar10_accuracy, improved_cifar10_accuracy, cifar100_accuracy]\n",
    "models = ['Simple CNN\\n(CIFAR-10)', 'Improved CNN\\n(CIFAR-10)', 'Improved CNN\\n(CIFAR-100)']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c844f",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've gone through the entire data science pipeline for image classification:\n",
    "\n",
    "1. We loaded and explored the CIFAR-10 and CIFAR-100 datasets\n",
    "2. We analyzed the datasets to understand their characteristics\n",
    "3. We implemented data augmentation to improve model generalization\n",
    "4. We built a simple CNN model and trained it on CIFAR-10\n",
    "5. We visualized the learned filters to understand what the model is learning\n",
    "6. We improved our model with additional layers and techniques\n",
    "7. We adapted our model to work with the more complex CIFAR-100 dataset\n",
    "8. We compared the performance of our different models\n",
    "\n",
    "Some key takeaways:\n",
    "- CNNs are powerful for image classification tasks\n",
    "- Data augmentation helps improve model generalization\n",
    "- Deeper models can capture more complex patterns\n",
    "- As the number of classes increases (CIFAR-10 to CIFAR-100), the classification task becomes more challenging\n",
    "\n",
    "## 11. Bonus Challenges\n",
    "\n",
    "If you've completed the notebook and want to explore further, here are some bonus challenges:\n",
    "\n",
    "1. Try implementing a state-of-the-art CNN architecture like ResNet or EfficientNet\n",
    "2. Experiment with different learning rates and optimizers\n",
    "3. Implement learning rate scheduling to improve training\n",
    "4. Use transfer learning with a pre-trained model\n",
    "5. Implement and compare different data augmentation techniques\n",
    "6. Visualize the feature maps at different layers of your CNN\n",
    "7. Implement and use techniques like Grad-CAM to visualize what your model is focusing on "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
